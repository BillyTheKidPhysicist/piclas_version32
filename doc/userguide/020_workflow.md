\hypertarget{workflow}{}

# Workflow \label{chap:workflow}

In this chapter, the complete process of setting up a simulation with **PICLas** is detailed.

## Mesh generation with HOPR

**PICLas** utilizes computational meshes from the high order preprocessor **HOPR** (available under GPLv3 at [https://www.hopr-project.org](https://www.hopr-project.org)) in HDF5 format. The design philosophy is that all tasks related to mesh organization, different input formats and the construction of high order geometrical mappings are separated from the *parallel* simulation code. These tasks are implemented most efficiently in a *serial* environment. The employed mesh format is designed to make the parallel read-in process as simple and fast as possible. For details concerning the mesh format please refer to the [HOPR HDF5 Curved Mesh Format Documentation](https://www.hopr-project.org/upload/e/e6/MeshFormat.pdf). Installation instructions can be found [here](https://www.hopr-project.org/index.php/Installation).

Using **HOPR**, simple, structured meshes can be directly created using an [in-built mesh generator.](https://www.hopr-project.org/index.php/Inbuilt_Mesh_Generators) A number of strategies to create curved boundaries are also included in HOPR. More complex geometries can be treated by importing meshes generated by external mesh generators in CGNS or GMSH format ([Example parameter file](https://www.hopr-project.org/index.php/External_Meshes)).

The basic command for either mesh generation or conversion of an extermal mesh is

~~~~~~~
hopr hopr.ini
~~~~~~~

Note that the path to the **HOPR** executable is omitted in the command (see \ref{sec:installation_directory}).

## Compiler options \label{sec:compileroptions}
This section describes the main configuration options which can be set when building **PICLas** using CMake. 
Some options are dependent on others being enabled (or disabled), so the available ones may change. 

The first set of options describe general CMake behaviour:

* ``CMAKE_BUILD_TYPE``: This statically specifies what build type (configuration) will be built in this build tree. Possible values are
    * Release: "Normal" execution.
    * Profile: Performance profiling using gprof.
    * Debug: Debug compiler for detailed error messages during code development.
    * SANI: Sanitizer compiler for even more detailed error messages during code development.
    
* ``CMAKE_HOSTNAME``: This will display the host name of the machine you are compiling on.

* ``CMAKE_INSTALL_PREFIX``: If “make install” is invoked or INSTALL is built, this directory is prepended onto all install directories. This variable defaults to /usr/local on UNIX.

For some external libraries and programs that **PICLas** uses, the following options apply:

* ``CTAGS_PATH``: This variable specifies the Ctags install directory, an optional program used to jump between tags in the source file.

* ``PICLAS_BUILD_HDF5``: This will be set to ON if no prebuilt HDF5 installation was found on your machine. In this case a HDF5 version will be build and used instead.

* ``HDF5_DIR``: If you want to use a prebuilt HDF5 library that has been build using the CMake system, this directory should contain the CMake configuration file for HDF5 (optional).

## Solver settings

Before setting up a simulation, the code must be compiled with the desired parameters. The most important compiler options to be set are:

* ``PICLAS_TIMEDISCMETHOD``: Module selection
  * DSMC: Direct Simulation Monte Carlo
  * RK4: Time integration method Runge-Kutta
* ``PICLAS_EQNSYSNAME``: Equation system to be solved
  * maxwell:
  * poisson:
* ``PICLAS_POLYNOMIAL_DEGREE``: Defines the polynomial degree of the solution. The order of convergence follows as $N+1$. Each grid cell contains $(N+1)^3$ collocation points to represent the solution.
* ``PICLAS_NODETYPE``: The nodal collocation points used during the simulation
  * GAUSS:
  * GAUSS-LOBATTO:
* ``PICLAS_INTKIND8``: Enables simulations with particle numbers above 2 147 483 647

The options EQNSYSNAME, POLYNOMIAL_DEGREE and NODETYPE can be ignored for a DSMC simulation. For parallel computation the following flags should be configured:

* ``PICLAS_MPI``: Enabling parallel computation
* ``PICLAS_LOADBALANCE``: Enable load-balancing

All other options are set in the parameter file.

## Setup of parameter file(s)

The settings of the simulation are controlled through parameter files, which are given as arguments to the binary. In the case of PIC simulations the input of a single
parameter file (e.g. *parameter.ini*) is sufficient, while the DSMC method requires the input of a species parameter file (e.g. *DSMCSpecies.ini*). The most recent list of parameters can be found by invoking the help in the console:

    piclas --help

General parameters such the name of project (used for filenames), the mesh file (as produced by HOPR), end time of the simulation (in seconds) are:

    ProjectName=TestCase
    MeshFile=test_mesh.h5
    TEnd=1e-3

An overview of the parameters is also given in Chapter \ref{chap:parameterfile}. The options and underlying models are discussed in Chapter \ref{chap:features_models}. Due to the sheer number of parameters available, it is advisable to build upon an existing parameter file.

## Simulation

After the mesh generation, compilation of the binary and setup of the parameter files, the code can be executed by

    piclas parameter.ini [DSMCSpecies.ini]

The simulation may be restarted from an existing state file

    piclas parameter.ini [DSMCSpecies.ini] [restart_file.h5]
    
**Note:** When restarting from an earlier time (or zero), all later state files possibly contained in your directory are deleted!

After a successful simulation, state files will be written out in the HDF5 format preceded by the project name, file type (e.g. State, DSMCState, DSMCSurfState) and the time stamp:

    TestCase_State_001.5000000000000000.h5
    TestCase_DSMCState_001.5000000000000000.h5

### Parallel execution
The simulation code is specifically designed for (massively) parallel execution using the MPI library. For parallel runs, the code must be compiled with `PICLAS_MPI=ON`. Parallel execution is then controlled using `mpirun`

    mpirun -np [no. processors] piclas parameter.ini [DSMCSpecies.ini] [restart_file.h5]
    
The grid elements are organized along a space-filling curved, which gives a unique one-dimensional element list. In a parallel run, the mesh is simply divided into parts along the space filling curve. Thus, domain decomposition is done *fully automatic* and is not limited by e.g. an integer factor between the number of cores and elements. The only limitation is that the number of cores may not exceed the number of elements.

## Post-processing \label{sec:postiVisu}

**PICLas** comes with a tool for visualization. The h5piclas2vtk tool converts the HDF5 files generated by **PICLas** to the binary VTK format, readable by many visualization tools like ParaView and VisIt. The tool is executed by

~~~~~~~
h5piclas2vtk [posti.ini] output.h5
~~~~~~~

Multiple HDF5 files can be passed to the h5piclas2vtk tool at once. The (optional) runtime parameters to be set in `posti.ini` are given in Section \ref{sec:tools_posti}.